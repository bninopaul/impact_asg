{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification\n",
    "#### Ni√±o Paul Batanay\n",
    "------\n",
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "data = np.loadtxt(\"seeds_dataset.txt\")\n",
    "\n",
    "#split it to train and test set\n",
    "test_ind = np.random.choice(np.arange(len(data)), size=40)\n",
    "train_ind = np.array([i for i in np.arange(len(data)) if i not in test_ind ])\n",
    "\n",
    "train = data[train_ind]\n",
    "test = data[test_ind]\n",
    "\n",
    "X_train, y_train = train[:,:-1], train[:,-1]\n",
    "X_test, y_test = test[:,:-1], test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the mean and variance for each feature of each class label in the training set\n",
    "mean1, covar1 = np.mean(X_train[y_train==1], axis=0), np.diag(np.var(X_train[y_train==1], axis=0))\n",
    "mean2, covar2 = np.mean(X_train[y_train==2], axis=0), np.diag(np.var(X_train[y_train==2], axis=0))\n",
    "mean3, covar3 = np.mean(X_train[y_train==3], axis=0), np.diag(np.var(X_train[y_train==3], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a function for the log_likelihood, we omit the uniform prior p(c_i) since it is constant across classes\n",
    "log_lik = lambda x,mean,covar: multivariate_normal(mean=mean, cov=covar).logpdf(x)\n",
    "log_like_test = np.column_stack((log_lik(X_test, mean1, covar1),\n",
    "                                 log_lik(X_test, mean2, covar2),\n",
    "                                 log_lik(X_test, mean3, covar3)))\n",
    "\n",
    "preds = np.argmax(log_like_test, axis=1)+1 #since indexing starts at 0\n",
    "\n",
    "#get the accuracy\n",
    "acc = np.mean(preds==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#equivalent naive bayes in sklearn\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "preds_sk = nb_classifier.predict(X_test)\n",
    "acc_sk = np.mean(preds_sk==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(preds==preds_sk) #same prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class naiveBayes(object):\n",
    "    \"\"\"\n",
    "    This class performs Naive Bayes classification for word-count document -\n",
    "    features.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data=None, train_labels=None):\n",
    "        \"\"\"\n",
    "        Initialize a Naive Bayes classifier.\n",
    "        \"\"\"\n",
    "        self.X = train_data\n",
    "        self.y = train_labels\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        \"\"\"\n",
    "        Fit the parameters according to the labeled training data (X,Y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "        Each row is the word-count vector for one of the documents\n",
    "        Y : ndarray of shape (n_samples,)\n",
    "        Gives the class label for each instance of training data. -\n",
    "        Assume class labels are in {0,1,...,k-1} where k is the -\n",
    "        number of classes.\n",
    "        \"\"\"\n",
    "        # get prior class probabilities P(c_i)\n",
    "        # (you may wish to store these as a length k vector as a class attribute)\n",
    "        # get (smoothed) word-class probabilities\n",
    "        # (you may wish to store these in a (k, n_features) matrix as a class attribute)\n",
    "        self.classes = np.unique(Y)\n",
    "        self.prob_c = np.array([np.mean(Y==i) for i in self.classes])\n",
    "        self.prob_word = np.array([(np.sum(X[Y==i], axis=0)+1)/(np.sum(X[Y==i])+X.shape[1])\n",
    "                                   for i in self.classes])\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels of a set of test data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "        The test data\n",
    "        Returns\n",
    "        -------\n",
    "        Y : ndarray of shape (n_samples,)\n",
    "        Gives the classification of each row in X\n",
    "        \"\"\"\n",
    "        log_lik = np.log(self.prob_c) + np.dot(X, np.log(self.prob_word).T)\n",
    "        preds = np.argmax(log_lik, axis=1).astype(float)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"SpamFeatures.txt\")\n",
    "labels = np.loadtxt(\"SpamLabels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split it to train and test set\n",
    "test_ind = np.random.choice(np.arange(len(data)), size=500)\n",
    "train_ind = np.array([i for i in np.arange(len(data)) if i not in test_ind])\n",
    "\n",
    "X_train, y_train = data[train_ind], labels[train_ind]\n",
    "X_test, y_test = data[test_ind], labels[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4691, 8167) (4691,)\n",
      "(500, 8167) (500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = naiveBayes(X_train, y_train)\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict(X_test)\n",
    "\n",
    "acc = np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95599999999999996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sklearn naive bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "preds_sk = nb.predict(X_test)\n",
    "acc_sk = np.mean(preds_sk==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95599999999999996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(preds==preds_sk) #same prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:impact]",
   "language": "python",
   "name": "conda-env-impact-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
