{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Code the Random Forest Classifier (Bagging Classifier version 2)\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "    for j = 1, ..., K (number for weak trees)\n",
    "        construct a tree ($f_j$) but every time you look for a split you only consider p randomly sampled $p$ features.\n",
    "        if no possible split:\n",
    "            stop\n",
    "    end\n",
    "    average $f_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODIFIED VERSION OF DECISION TREE CLASSIFIER FROM PREVIOUS LAB\n",
    "(add number of features to consider in the split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we modify the Node class object we used in the\n",
    "#classification tree to accept an input num_feat\n",
    "#which will be used to consider how many features \n",
    "#to use in the split.\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Creates a classification tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, feat_type, num_feat, cur_depth=1, max_depth=10, tol=0.2):\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "        X: matrix(2d numpy array) in num_samples x num_features format\n",
    "        y: labels (1d numpy array)\n",
    "        feat_type: list of strings indicating the type of features (categorical or real),\n",
    "                    must be have length equal to num_features\n",
    "        num_feat: number of features to consider in splitting for every node\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feat_type = feat_type\n",
    "        self.cur_depth = cur_depth\n",
    "        if max_depth<cur_depth:\n",
    "            raise ValueError(\"Maximum depth should be greater than current_depth\")\n",
    "        self.max_depth = max_depth\n",
    "        self.tol = tol\n",
    "        self.num_feat = num_feat\n",
    "        \n",
    "        def gini_impurity(y, classes):\n",
    "            \"\"\"\n",
    "            y: array(num_samples)\n",
    "            classes: array of unique elements of classes\n",
    "            \"\"\"\n",
    "            percent = np.array([np.mean(y==k) if np.any(y) else 0 for k in classes])\n",
    "            gini = 1 - np.sum(percent**2)\n",
    "            return gini\n",
    "        \n",
    "        #splitting of D to D1 and D2 given the index of the attribute p and value x\n",
    "        def split(X, y, p, x, feat_type):\n",
    "            \"\"\"\n",
    "            X: matrix(num_samples x num_features)\n",
    "            y: array(num_samples,)\n",
    "            p: (attribute) column index\n",
    "            x: value of the attribute used as a basis of splitting\n",
    "            feat_type: list of strings indicating if the feature is real or categorical\n",
    "            \"\"\"\n",
    "            if feat_type==\"categorical\": \n",
    "                Xsplit1, ysplit1 = X[X[:,p]==x], y[X[:,p]==x]\n",
    "                Xsplit2, ysplit2 = X[X[:,p]!=x], y[X[:,p]!=x]\n",
    "            elif feat_type==\"real\":\n",
    "                Xsplit1, ysplit1 = X[X[:,p]<=x], y[X[:,p]<=x]\n",
    "                Xsplit2, ysplit2 = X[X[:,p]>x], y[X[:,p]>x]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid feat_type\")\n",
    "            return (Xsplit1, Xsplit2), (ysplit1, ysplit2)\n",
    "        \n",
    "        #information gain\n",
    "        def info_gain(X, y, p, x, feat_type):\n",
    "            \"\"\"\n",
    "            X: matrix(num_samples x num_features)\n",
    "            y: array(num_samples,)\n",
    "            p: (attribute) column index\n",
    "            x: value of the attribute used as a basis of splitting\n",
    "            feat_type: list of strings indicating if the feature is real or categorical\n",
    "            \"\"\"\n",
    "            classes = np.unique(y)\n",
    "            #split \n",
    "            Xsplits, ysplits = split(X, y, p, x, feat_type)\n",
    "            score = gini_impurity(y, classes) - np.sum([(len(ysplits[i])/len(y))*gini_impurity(ysplits[i], classes)\n",
    "                                                        for i in range(2) if Xsplits[i].any()])\n",
    "            return score\n",
    "        \n",
    "        def optimal_split(X, y, feat_type, num_feat):\n",
    "            \"\"\"\n",
    "            X: matrix(num_samples x num_features)\n",
    "            y: array(num_samples,)\n",
    "            feat_type: list of strings indicating if the feature is real or categorical\n",
    "            num_feat: number of features to consider in splitting\n",
    "            \"\"\"\n",
    "            #maximize info gain\n",
    "            m,n = X.shape\n",
    "            best_ig = 0 #best info gain (find the maximum)\n",
    "            best_j = 0 #best coordinate point index to create x\n",
    "            best_i = 0 #best attribute index\n",
    "            feats = np.random.choice(np.arange(n), num_feat, replace=False)\n",
    "            for i in feats:\n",
    "                for j in range(m):\n",
    "                    x = X[j,i]\n",
    "                    (X1, X2), (y1, y2) = split(X, y, i, x, feat_type[i])\n",
    "                    ig = info_gain(X, y, i, x, feat_type[i])\n",
    "                    if ig>best_ig:# and np.any(X1) and np.any(X2): #make sure X1 and X2 are nonempty\n",
    "                        best_ig, best_j, best_i = ig, j, i\n",
    "\n",
    "            return split(X, y, best_i, X[best_j, best_i], feat_type[best_i]), best_i, X[best_j, best_i]\n",
    "        \n",
    "        ((X1, X2), (y1, y2)), attr, xval = optimal_split(self.X, self.y,\n",
    "                                                         self.feat_type, self.num_feat)\n",
    "        \n",
    "        self.attr = attr\n",
    "        self.xval = xval\n",
    "        classes = np.sort(np.unique(y))\n",
    "         \n",
    "        gin_imp1 = gini_impurity(y1, classes)\n",
    "        gin_imp2 = gini_impurity(y2, classes)\n",
    "        \n",
    "        if  ((self.cur_depth == self.max_depth) |\\\n",
    "                (gin_imp1<tol) | (gin_imp2<tol)| (not np.any(X1)) | (not np.any(X2))):\n",
    "            self.asg_label = np.argmax([np.sum(y==i) for i in classes])\n",
    "        else:\n",
    "            self.leftchild = Node(X1, y1, self.feat_type, num_feat=self.num_feat, cur_depth=self.cur_depth+1,\n",
    "                                  max_depth=self.max_depth, tol=self.tol)\n",
    "            self.rightchild = Node(X2, y2, self.feat_type, num_feat=self.num_feat, cur_depth=self.cur_depth+1,\n",
    "                                   max_depth=self.max_depth, tol=self.tol)\n",
    "    def print_tree(self):\n",
    "        \n",
    "        print(\"==\"*(self.cur_depth-1)+\">\"+\"Attr:{}, Xval:{}\".format(self.attr, self.xval))\n",
    "        \n",
    "        if not hasattr(self, \"asg_label\"):\n",
    "            self.leftchild.print_tree()\n",
    "            self.rightchild.print_tree()\n",
    "        else:\n",
    "            print(\"==\"*self.cur_depth+\">\"+\"Decision: Assigned label:{}\".format(self.asg_label))\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        if hasattr(self, \"asg_label\"):\n",
    "            return self.asg_label\n",
    "        else:\n",
    "            if self.feat_type[self.attr]==\"categorical\":\n",
    "                if x[self.attr]==self.xval:\n",
    "                    return self.leftchild.predict(x)\n",
    "                else:\n",
    "                    return self.rightchild.predict(x)\n",
    "            elif self.feat_type[self.attr]==\"real\":\n",
    "                if x[self.attr]<=self.xval:\n",
    "                    return self.leftchild.predict(x)\n",
    "                else:\n",
    "                    return self.rightchild.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627, 3) (627,)\n",
      "(567, 3) (567,)\n"
     ]
    }
   ],
   "source": [
    "#load the titanic dataset\n",
    "data = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "#preprocessing\n",
    "data.drop([\"Name\", \"Boat\", \"Body\", \"Ticket\", \"Cabin\", \"home.dest\", \"Embarked\"], axis=1, inplace=True)\n",
    "data[\"Sex\"] = [1 if i==\"male\"  else 0 for i in data[\"Sex\"]]\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "X = np.array(data[[\"Pclass\", \"Sex\", \"Age\"]])\n",
    "feat_type = [\"categorical\", \"categorical\", \"real\"]\n",
    "y = np.array(data[\"Survived\"])\n",
    "\n",
    "#train and test split\n",
    "train_ind = np.random.choice(np.arange(len(X)), size=int(0.6*len(X)))\n",
    "test_ind = np.array([i for i in np.arange(len(X)) if i not in train_ind ])\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = X[train_ind], y[train_ind], X[test_ind], y[test_ind]\n",
    "print(Xtrain.shape, ytrain.shape)\n",
    "print(Xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one weak tree using our Decision tree classifier\n",
    "tree = Node(Xtrain, ytrain, feat_type, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_train = np.array([tree.predict(i) for i in Xtrain])\n",
    "acc_train = np.mean(preds_train==ytrain)\n",
    "\n",
    "preds_test = np.array([tree.predict(i) for i in Xtest])\n",
    "acc_test = np.mean(preds_test==ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791068580542 0.776014109347\n"
     ]
    }
   ],
   "source": [
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=3, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's compare with sklearn's Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(max_features=3)\n",
    "dt.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_train = dt.predict(Xtrain)\n",
    "acc_train = np.mean(preds_train==ytrain)\n",
    "\n",
    "preds_test = dt.predict(Xtest)\n",
    "acc_test = np.mean(preds_test==ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.897926634769 0.756613756614\n"
     ]
    }
   ],
   "source": [
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST CLASSIFIER THAT USES THE MODIFIED VERSION OF DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nodes():\n",
    "    \"\"\"\n",
    "    Creates a random forest classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, feat_type, num_feat=None, num_trees=None, \n",
    "                 cur_depth=1, max_depth=10, tol=0.2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feat_type = feat_type\n",
    "        if num_feat==None:\n",
    "            #for classification, rule of thumb is sqrt\n",
    "            #of the dimension of features\n",
    "            self.num_feat = int(np.sqrt(X.shape[1])) \n",
    "        else:\n",
    "            self.num_feat = num_feat\n",
    "        if num_trees==None:\n",
    "            #rule of thumb is range(100, 1000) trees\n",
    "            self.num_trees = 500\n",
    "        else:\n",
    "            self.num_trees = num_trees\n",
    "        \n",
    "        #build weak classification trees\n",
    "        self.trees = []\n",
    "        for j in range(self.num_trees):\n",
    "            tree = Node(self.X, self.y, self.feat_type, self.num_feat)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        self.preds = []\n",
    "        for j in self.trees:\n",
    "            self.preds.append(j.predict(x))\n",
    "        return int(np.mean(self.preds)>=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = Nodes(Xtrain, ytrain, feat_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_train = np.array([forest.predict(i) for i in Xtrain])\n",
    "acc_train = np.mean(preds_train==ytrain)\n",
    "\n",
    "preds_test = np.array([forest.predict(i) for i in Xtest])\n",
    "acc_test = np.mean(preds_test==ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800637958533 0.768959435626\n"
     ]
    }
   ],
   "source": [
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use sklearn's Random Forest Classifier\n",
    "num_trees = 500\n",
    "num_feat = int(np.sqrt(Xtrain.shape[1])) \n",
    "rf = RandomForestClassifier(n_estimators=num_trees, max_features=num_feat)\n",
    "rf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_train = rf.predict(Xtrain)\n",
    "acc_train = np.mean(preds_train==ytrain)\n",
    "\n",
    "preds_test = rf.predict(Xtest)\n",
    "acc_test = np.mean(preds_test==ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.897926634769 0.749559082892\n"
     ]
    }
   ],
   "source": [
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:impact]",
   "language": "python",
   "name": "conda-env-impact-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
